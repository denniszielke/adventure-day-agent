{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 - Efficiency\n",
    "\n",
    "This lab is about making sure you use less tokens - because tokens is the \"currency\" for AI requests and we want to save money! \n",
    "The way we tackle it in this lab is by caching questions and answers - so you don't need to ask your LLM all the time. To make this a little more sophisticated we are generating vectors for the questions - and therefore cache the semantics of the questions, not just the string.\n",
    "\n",
    "Below you will find a sample how to do this.\n",
    "It's your job to build this caching into your api so you can reduce the number of tokens used. Are there other optimizations you could think of?\n",
    "\n",
    "\n",
    "\n",
    "If not already done run this in the top level folder:\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "if load_dotenv():\n",
    "    print(\"Found Azure OpenAI API Base Endpoint: \" + os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "else: \n",
    "    print(\"Azure OpenAI API Base Endpoint not found. Have you configured the .env file?\")\n",
    "    \n",
    "API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "API_VERSION = os.getenv(\"OPENAI_API_VERSION\")\n",
    "RESOURCE_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_VERSION\")\n",
    ")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\")\n",
    "model_name = os.getenv(\"AZURE_OPENAI_COMPLETION_MODEL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the object model for receiving questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class QuestionType(str, Enum):\n",
    "    multiple_choice = \"multiple_choice\"\n",
    "    true_or_false = \"true_or_false\"\n",
    "    popular_choice = \"popular_choice\"\n",
    "    estimation = \"estimation\"\n",
    "\n",
    "class Ask(BaseModel):\n",
    "    question: str | None = None\n",
    "    type: QuestionType\n",
    "    correlationToken: str | None = None\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    answer: str\n",
    "    correlationToken: str | None = None\n",
    "    promptTokensUsed: int | None = None\n",
    "    completionTokensUsed: int | None = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the number of tokens\n",
    "tiktoken is a library which allows you to get the number of tokens. This will allow you to check how much tokens you've been using.\n",
    "Ensure you pick the correct encoding for your model based on this list. https://github.com/openai/tiktoken/blob/c0ba74c238d18b4824c25f3c27fc8698055b9a76/tiktoken/model.py#L20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_tokens_from_string(string: str, encoding_name: str='p50k_base') -> int:\n",
    "    \"\"\"Returns the number of tokens in a text by a given encoding.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    return len(encoding.encode(string))\n",
    "\n",
    "number_of_tokens=get_num_tokens_from_string(\"Hello, Azure AI Adventure Day!\")\n",
    "print(f\"Number of tokens in the string: {number_of_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use AI Search for semantic caching\n",
    "The snippets below show you how we cache the semantic meaning of questions into Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use an embeddingsmodel to create embeddings\n",
    "def get_embedding(text, model=os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")):\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchFieldDataType,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    SearchIndex\n",
    "\n",
    ")\n",
    "\n",
    "credential = AzureKeyCredential(os.environ[\"AZURE_AI_SEARCH_KEY\"]) if len(os.environ[\"AZURE_AI_SEARCH_KEY\"]) > 0 else DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new index to store questions and answers - and the vector which represents the semantic of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_name = \"question-semantic-index\"\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=os.environ[\"AZURE_AI_SEARCH_ENDPOINT\"], \n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "# Create a search index with the fields and a vector field which we will fill with a vector based on the overview field\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "    SearchableField(name=\"question\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"answer\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),\n",
    "]\n",
    "\n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\"\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configure the semantic search configuration \n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"question-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"question\"),\n",
    "        keywords_fields=[SemanticField(field_name=\"answer\")],\n",
    "        content_fields=[SemanticField(field_name=\"question\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields,\n",
    "                    vector_search=vector_search, semantic_search=semantic_search)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import some test data. As you see below the test data set contains 3 questions - all asking for the same with different words. So the semantics are the same, but it's not a word by word match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "questions = [{\n",
    "        \"question\": \"Which actor plays Tony Stark in the Marvel movies?\",\n",
    "        \"answer\": \"Robert Downey Jr.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"In the Marvel Cinematic Universe, who is the actor that brings Tony Stark to life?\",\n",
    "        \"answer\": \"Robert Downey Jr.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who brings the character of Tony Stark to life in the Marvel Cinematic Universe?\",\n",
    "        \"answer\": \"Robert Downey Jr.\"\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes, we put those questions into the index. To do this, we create a vector for all questions which represent the meaning of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embedding for the question\n",
    "index = 1\n",
    "for question in questions:\n",
    "    question[\"id\"] = str(index)\n",
    "    question[\"vector\"] = get_embedding(question[\"question\"])\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put all those questions into our new index of Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# create new searchclient using our new index for the questions\n",
    "search_client = SearchClient(\n",
    "    endpoint=os.environ[\"AZURE_AI_SEARCH_ENDPOINT\"], \n",
    "    index_name=index_name,\n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "# upload question to vector store\n",
    "result = search_client.upload_documents(questions)\n",
    "print(f\"Successfully loaded {len(questions)} questions into Azure AI Search index.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a try - ask the same question again - this time with yet another sentence but with the same semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is Tony Stark in the MCU?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we don't just ask the LLM - instead we generate an embedding and search for the vector in our new index - and get the top 5 questions and answers and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import (\n",
    "    VectorizedQuery\n",
    ")\n",
    "\n",
    "# create a vectorized query based on the question\n",
    "vector = VectorizedQuery(vector=get_embedding(question), k_nearest_neighbors=5, fields=\"vector\")\n",
    "\n",
    "# create search client to retrieve movies from the vector store\n",
    "found_questions = list(search_client.search(\n",
    "    search_text=None,\n",
    "    query_type=\"semantic\",\n",
    "    semantic_configuration_name=\"question-semantic-config\",\n",
    "    vector_queries=[vector],\n",
    "    select=[\"question\", \"answer\"],\n",
    "    top=5\n",
    "))\n",
    "\n",
    "# print the found documents and the field that were selected\n",
    "for result in found_questions:\n",
    "    print(\"Question: {}\".format(result[\"question\"]))\n",
    "    print(\"Answer: {}\".format(result[\"answer\"]))\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have seen, we can get the correct answer for a question that was never asked the same way before, if we manage to cache questions and answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOUR Mission: \n",
    "Adjust the function below and reuse it in the main.py file later to deploy to Azure and to update your service. \n",
    "Ensure the answers provided are correct and in the correct format.\n",
    "\n",
    "- for incomingn questions, create a vector embedding\n",
    "- check if the answer is in the cache before \n",
    "- if yes, \n",
    "    - return the answer\n",
    "    - put the new question & answer in the cache as well\n",
    "- if no, \n",
    "    - reach out to the llm to get the answer. \n",
    "    - Then put the question & answer in the cache in case a similar question will come up again\n",
    "- measure the tokens used \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ask_question(ask: Ask):\n",
    "    \"\"\"\n",
    "    Ask a question\n",
    "    \"\"\"\n",
    "\n",
    "    # Send a completion call to generate an answer\n",
    "    print('Sending a request to openai')\n",
    "    start_phrase = ask.question\n",
    "    messages=  [{\"role\" : \"assistant\", \"content\" : start_phrase}]\n",
    "    number_of_tokens_in= get_num_tokens_from_string(messages[0]['content']);\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model = deployment_name,\n",
    "        messages =messages,\n",
    "    )\n",
    "\n",
    "    number_of_tokens_out= get_num_tokens_from_string(response.choices[0].message.content);\n",
    "\n",
    "    print(f\"Number of tokens in the input: {number_of_tokens_in}\")\n",
    "    print(f\"Number of tokens in the output: {number_of_tokens_out}\")\n",
    "    print('Total Tokens Used: ' + str(number_of_tokens_in + number_of_tokens_out))\n",
    "\n",
    "    answer = Answer(answer=response.choices[0].message.content)\n",
    "    answer.correlationToken = ask.correlationToken\n",
    "    answer.promptTokensUsed = response.usage.prompt_tokens\n",
    "    answer.completionTokensUsed = response.usage.completion_tokens\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this snippet to try your method with several questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ask = Ask(question=\"Who brings the character of Tony Stark to life in the Marvel Cinematic Universe? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\", type=QuestionType.multiple_choice)\n",
    "answer = await ask_question(ask)\n",
    "print('Answer:', answer)\n",
    "\n",
    "ask = Ask(question=\"In the Marvel Cinematic Universe, who is the actor that brings Tony Stark to life? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\", type=QuestionType.multiple_choice)\n",
    "answer = await ask_question(ask)\n",
    "print('Answer:', answer)\n",
    "\n",
    "ask = Ask(question=\"Which actor plays Tony Stark in the Marvel movies? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\", type=QuestionType.multiple_choice)\n",
    "answer = await ask_question(ask)\n",
    "print('Answer:', answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you transfer your code changes into main.py (or additional files). Then redeploy your container using this command.\n",
    "```\n",
    "bash ./azd-hooks/deploy.sh phase4 $AZURE_ENV_NAME\n",
    "```\n",
    "Make sure to provide the URL of your endpoint in the team portal!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
